{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b431524c",
   "metadata": {},
   "source": [
    "## Homework 1: ImageGPT\n",
    "\n",
    "Мы привыкли, что для работы с изображениями нужны сверточные сети. Но что, если взглянуть на задачу под другим углом? В этой работе мы отойдем от классического подхода и исследуем, как можно применить архитектуру **Трансформер** для генерации изображений. Для этого мы реализуем архитектуру **ImageGPT**, которая обрабатывает изображения так же, как классический GPT работает с текстом - как с последовательностью токенов.\n",
    "\n",
    "### Основная идея\n",
    "Эта идея впервые подробно была описана в работе [Generative Pretraining from Pixels](https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf) от **OpenAI**. Её авторы показали, что если представить изображение в виде последовательности, то модель способна обучиться не хуже, чем классические сверточные сети. \n",
    "\n",
    "Суть этой идеи в том, что вместо сверточных слоёв **ImageGPT** преобразует изображение в последовательность квантованных пикселей, которые выступают в роли токенов. Эта последовательность затем подаётся на вход GPT, и модель учится предсказывать следующий токен.\n",
    "\n",
    "### Задание\n",
    "Вам предстоит реализовать основные блоки архитектуры ImageGPT и обучить модель на датасете `CIFAR-10` для решения задачи генерации изображений.\n",
    "\n",
    "За выполнение домашнего задания можно получить до **10 баллов**. Для части заданий мы написали для вас скелет. Заполните в них пропуски, выделенные с помощью `...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debdb962-54df-4544-9c38-ce709d249c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from IPython.display import clear_output\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ad2471",
   "metadata": {},
   "source": [
    "### Задание 1: Dataset (0.5 балла)\n",
    "\n",
    "Для обучения нашей модели мы будем использовать популярный датасет `CIFAR-10`. Он состоит из $60 000$ цветных изображений размером $32\\times 32$ пикселя, разделенных на $10$ классов.\n",
    "\n",
    "Датасет уже за вас поделен на **50 000** обучающих и **10 000** тестовых изображений.\n",
    "\n",
    "Для удобства дальнейшей работы нам нужно получить датасет в формате Dataset из PyTorch. Проще всего получить из библиотеки `torchvision.datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2166af3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcbf996-7d81-45d7-94ec-762a9ac1cff5",
   "metadata": {},
   "source": [
    "- Создайте преобразования `transforms` с аугментациями `RandomCrop` и `RandomHorizontalFlip` для обучающего (`train_dataset`) и без аугментации для валидационного (`val_dataset`) датасетов.\n",
    "- Загрузите CIFAR-10 с соответствующими преобразованиями\n",
    "- Создайте DataLoaders для каждого датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4cebeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51883a2b",
   "metadata": {},
   "source": [
    "Прежде чем приступить к построению модели всегда полезно взглянуть на данные, с которыми предстоит работать. Это даёт общее представление о данных и позволяет убедиться, что всё загрузилось корректно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49a2f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings = {\n",
    "    0: 'airplane', 1: 'automobile', 2: 'bird', 3: 'cat', 4: 'deer', \n",
    "    5: 'dog', 6: 'frog', 7: 'horse', 8: 'ship', 9: 'truck'\n",
    "}\n",
    "\n",
    "# Select a random set of indices from the training dataset.\n",
    "indices = torch.randperm(len(train_dataset))[:40]\n",
    "\n",
    "fig, axes = plt.subplots(5, 8, figsize=(16, 10))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    image, label = train_dataset[indices[i]]  \n",
    "\n",
    "    image = image.permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "    ax.imshow(image)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f\"{mappings[label]}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f282894a-cf09-433e-90e1-29ae85070069",
   "metadata": {},
   "source": [
    "### Задание 2: Квантование изображений (1.5 балла)\n",
    "\n",
    "Теперь давайте разберёмся, как нам подготовить изображения для трансформера.  \n",
    "\n",
    "Трансформеры работают с **дискретными токенами**, но стандартные цифровые изображения обычно хранят значения пикселей в виде дискретных целых чисел от 0 до 255. То есть каждый пискель может иметь одно из $256^3\\approx 16.8$ миллиона возможных цветовых комбинаций. Если бы мы использовали каждую из этих комбинаций как отдельный токен для нашей модели, то у нас был бы просто огромный словарь, и вряд ли бы мы смогли обучить трансформер на таких данных.\n",
    "\n",
    "Решение этой проблемы лежит в квантовании. Обычно в PyTorch мы используем преобразование `torchvision.transforms.ToTensor()`. Оно не только конвертирует изображение в тензор PyTorch, но и нормализует значения пикселей из диапазона $[0, 255]$ в диапазон $[0.0, 1.0]$. В итоге, наши данные становятся непрерывными.\n",
    "\n",
    "Чтобы сделать их дискретными, в ImageGPT предложили метод квантования пикселей с помощью `K-Means`. Вместо того чтобы представлять каждый пиксель его `float` значением, мы группируем похожие цвета в небольшое, заранее заданное количество **цветовых кластеров** или **кодовых слов** (**codebook**). Каждый пиксель изображения затем заменяется целочисленным индексом того цветового кластера, к которому он наиболее близок.\n",
    "\n",
    "Кратко процесс выглядит так:\n",
    "\n",
    "1) **Собираем все пиксели из нашего датасета**.\n",
    "\n",
    "2) **Применяем алгоритм K-Means к этим пикселям с желаемым количеством кластеров**.\n",
    "\n",
    "В результате мы получаем $K$ кластеров. Каждый из них представляет собой усреднённый цвет (центроид) для своей группы пикселей. Эти центроиды и станут нашим словарём для дальнейшего квантования изображений.\n",
    "\n",
    "Теперь давайте обучим `K-Means` на наших данных с `n_clusters=512` и сохраним кластеры в формате `.npy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15500058-4beb-44b2-9bc6-27f1e1070ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cifar10_centroids(n_clusters=512, save_path=\"path/to/save\"):\n",
    "    \n",
    "    print(\"Loading CIFAR-10...\")\n",
    "    dataset = ... # TODO: pay attention to transforms\n",
    "\n",
    "    print(\"Extracting pixels...\")\n",
    "    all_pixels = []\n",
    "    \n",
    "    for img, _ in dataset:\n",
    "        img_np = ...\n",
    "        all_pixels.append(...) \n",
    "        \n",
    "    all_pixels = np.concatenate(all_pixels, axis=0)\n",
    "\n",
    "    print(f\"Clustering with KMeans (n_clusters={n_clusters})...\")\n",
    "    kmeans = ...\n",
    "    centroids = kmeans.cluster_centers_\n",
    "\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    np.save(save_path, centroids)\n",
    "    print(f\"Centroids saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513555ec-ad7f-4720-9895-fac40a436efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_cifar10_centroids(n_clusters=512, save_path=\"clusters/cifar10_clusters.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cbc57f-f4f0-400d-a072-86aef1246c9f",
   "metadata": {},
   "source": [
    "Также во время обучения нам понадобятся еще несколько вспомогательных функций:\n",
    "\n",
    "- **`img_to_seq`**: преобразует двумерное изображение в одномерную последовательность токенов. Тут используем формат сначала размер последовательности, потом размер батча.\n",
    "\n",
    "- **`quantize`**: принимает на вход изображение и преобразует его в изображение, где каждый пиксель заменен индексом ближайшего центроида из нашего словаря.\n",
    "\n",
    "- **`unquantize`**: выполняет обратную операцию, которая принимает квантованное изображение и восстанавливает его в пиксельное представление"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf2d81d-686d-468b-bec9-523ae6878264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_to_seq(x):\n",
    "    # Reshape x to [Batch_Size, H*W] and then transpose to [H*W, Batch_Size]\n",
    "    # Use .contiguous() to avoid view() and inplace-ops errors after transpose.\n",
    "    x = ...\n",
    "    return x\n",
    "\n",
    "def quantize(x, centroids):\n",
    "    b, c, h, w = x.shape\n",
    "    \n",
    "    # Calculate Euclidean distance using (a-b)^2 = a^2 - 2ab + b^2 \n",
    "    # and find the index of the closest centroid for each pixel\n",
    "   \n",
    "    return x.view(b, h, w)      \n",
    "\n",
    "def unquantize(x, centroids):\n",
    "    \n",
    "    return ... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe239db",
   "metadata": {},
   "source": [
    "### Задание 3: Embeddings (1.5 балла)\n",
    "\n",
    "Чтобы трансформер мог работать с изображениями, их нужно преобразовать в понятный для него формат. В текстовых моделях слова сначала превращаются в  токены, а затем — в вектора эмбеддингов, в которых хранится \"смысл слов\".\n",
    "\n",
    "С изображениями мы поступаем так же, но вместо слов мы работаем с пикселями. Мы уже отметили, что каждый пиксель в ImageGPT рассматривается как отдельный токен, который получает индекс из нашего словаря, созданного с помощью K-Means. Однако трансформеру нужны не просто эти индексы, а их векторные представления, которые он может обрабатывать. В процессе обучения эти векторы постепенно учатся отражать некоторые связи между пикселями, что позволяет модели понимать изображение.\n",
    "\n",
    "В ImageGPT нам потребуются несколько типов эмбеддингов для кодирования входных изображений:\n",
    "\n",
    "- **`token_embeddings`**: преобразуют индексы квантованных пикселей в вектора\n",
    "\n",
    "- **`position_embeddings`**: кодируют информацию о позиции каждого пикселя, это позволяет модели улавливать пространственные отношения между ними\n",
    "\n",
    "- **`class_embedding`**: преобразует индекс класса в вектор и выступает в роли условного стартового токена, который сообщает модели, изображение какого именно класса нужно генерировать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ea656a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, max_positions, num_classes):\n",
    "        super().__init__()\n",
    "        self.token_embeddings = ...\n",
    "        self.position_embeddings = ...\n",
    "        self.class_embedding = ...\n",
    "\n",
    "    def forward(self, x, cls_label):\n",
    "        \n",
    "        tok_emb = ...\n",
    "        cls_token_emb = ...\n",
    "\n",
    "        full_seq = ... # concat cls_token\n",
    "\n",
    "        pos_ids = ...\n",
    "        pos_emb = ...\n",
    "\n",
    "\n",
    "        # Return final embedding sequence:\n",
    "        # Add positional information to full_seq\n",
    "        \n",
    "        return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2387087d-faa6-42bc-8573-f17ffb312f4e",
   "metadata": {},
   "source": [
    "### Задание 4: Decoder Block (1.5 балла)\n",
    "\n",
    "Теперь нам нужно собрать основные блоки нашей модели — блоки декодера. В каждом таком блоке последовательно будут выполняются следующие операции:\n",
    "\n",
    "- **`LayerNorm`**\n",
    "- **`MultiHeadAttention`**\n",
    "- **`Residual Connection`**\n",
    "- **`LayerNorm`**\n",
    "- **`MLP`**:\n",
    "  - **Linear**: $embed\\; dim$ -> $4 \\times embed\\; dim$\n",
    "  - **GELU Activation**\n",
    "  - **Linear**: $(4 \\times embed\\; dim)$ -> $embed\\; dim$\n",
    "- **`Residual Connection`**\n",
    "\n",
    "Поскольку наша модель является генеративной и предсказывает следующий элемент в последовательности, вам потребуется использовать **Masked Self-Attenion**.\n",
    "\n",
    "Вы можете вопользоваться готовой реализацией **Multi-Head Attention** из Pytroch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f9d32e-c4aa-4015-8553-2ff6e71c6167",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.ln1 = ...\n",
    "        self.attn = ...  \n",
    "        self.ln2 = ...  \n",
    "        self.mlp = nn.Sequential(\n",
    "            ... ,  \n",
    "            ... ,  \n",
    "            ...  \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a462ffbe-93ff-45cf-b124-1b855f285b9a",
   "metadata": {},
   "source": [
    "### Задание 5: Final Model (1 балл)\n",
    "\n",
    "Теперь, когда у нас есть все компоненты, мы можем собрать их вместе, чтобы построить итоговую модель ImageGPT.\n",
    "\n",
    "Итоговая архитектура выглядит так:\n",
    "\n",
    "- **`Embedding`**: преобразует входные токены в векторы\n",
    "\n",
    "- **`Blocks`**: использует `num_layers` блоков `DecoderBlock`, которые последовательно обрабатывают наши данные\n",
    "\n",
    "- **`Final LayerNorm`**: нормализует выходные данные перед финальным предсказанием\n",
    "\n",
    "- **`Head`**: преобразует обработанные векторы в предсказания\n",
    "\n",
    "- **`Centroids`**: храним центроиды как параметр модели, они необходимы для обратного преобразования токенов в реальные цвета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a7821a-dd3e-4323-b895-619de85dae92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, max_positions, centroids_path, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = ...\n",
    "        self.blocks = ...\n",
    "        self.ln_f = ...\n",
    "        self.head = ...\n",
    "\n",
    "        centroids = torch.tensor(np.load(centroids_path), dtype=torch.float32)\n",
    "        self.register_buffer('centroids', centroids)\n",
    "\n",
    "    def forward(self, x, y):        \n",
    "        # Compute embeddings\n",
    "        h = ...  \n",
    "        \n",
    "        for block in self.blocks:\n",
    "            h = ...  \n",
    "        \n",
    "        # Apply final LayerNorm\n",
    "        h = ...  \n",
    "        \n",
    "        # Project to vocabulary logits\n",
    "        output = ...\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2d7ddf-6666-42e1-bbf5-fa6b2aee9179",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids_path=\"clusters/cifar10_clusters.npy\"\n",
    "\n",
    "model = ImageGPT(\n",
    "    centroids_path=centroids_path,\n",
    "    embed_dim=512,\n",
    "    num_heads=8,\n",
    "    num_layers=24,\n",
    "    max_positions=32*32,\n",
    "    vocab_size=512,\n",
    "    num_classes=10\n",
    "    )\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7bbfba-a8e6-44cf-88d6-d5878b321da1",
   "metadata": {},
   "source": [
    "### Задание 6: Sampling (1.5 балла)\n",
    "\n",
    "Для генерации новых изображений мы будем использовать уже обученную модель ImageGPT. Давайте напишем функцию **`sample`**, которая будет отвечать за это.\n",
    "\n",
    "Она будет принимать на вход следующие параметры:\n",
    "\n",
    "- **`model`**: наша обученная модель ImageGPT.\n",
    "\n",
    "- **`length`**: число шагов генерации\n",
    "\n",
    "- **`context`**: начальная последовательность пикселей, если мы хотим дорисовать изображение. Если её нет, генерация начнётся с нуля.\n",
    "\n",
    "- **`class_label`**: метка класса для генерации по заданному условию\n",
    "\n",
    "- **`temperature`**: параметр, который регулирует уровень случайности при генерации\n",
    "\n",
    "- **`num_samples`**: количество изображений, которые нужно сгенерировать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccd11eb-0ec3-446a-92b4-c9f34664df1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, length, class_label, num_samples=1, device='cuda',\n",
    "           context=None, temperature=1.0):\n",
    "\n",
    "    # expand it so that each generated image has a class label\n",
    "    class_label = ...\n",
    "    \n",
    "    if context is None or context.size(0) == 0:\n",
    "        context = torch.empty((0, num_samples), dtype=torch.long, device=device)\n",
    "    else:\n",
    "        if context.dim() == 1:\n",
    "            # Add a new dimension to make it compatible with the model's input format.\n",
    "            ...\n",
    "        # Repeat the context to generate multiple images from the same starting sequence\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Implement the generation loop here\n",
    "        ...\n",
    "\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b018484-457a-4ff4-b574-ef23a305b32d",
   "metadata": {},
   "source": [
    "### Задание 7: Training and Validation Loop (1.5 балла)\n",
    "\n",
    "После того как все компоненты модели собраны, мы можем приступить к обучению. Здесь вамм нужно реализовать основные функции для тренировки и валидации нашей модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d414a6-5940-4331-8886-649b78db4f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, optimizer, criterion, scheduler, device):\n",
    "    \n",
    "    model.train()  \n",
    "    total_loss = 0.0\n",
    "\n",
    "    for images, labels in tqdm(train_loader, desc=\"Train\"):\n",
    "        images, labels = ...  \n",
    "\n",
    "        logits = ...  \n",
    "\n",
    "        loss = ...  \n",
    "\n",
    "        total_loss += ...  \n",
    "\n",
    "    # Compute and return average loss for this epoch\n",
    "    \n",
    "    return ...  \n",
    "\n",
    "\n",
    "def validate_one_epoch(model, val_loader, criterion, device):\n",
    "\n",
    "    model.eval()  \n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():  \n",
    "        for images, labels in tqdm(val_loader, desc=\"Val\"):\n",
    "            images, labels = ...  \n",
    "\n",
    "            logits = ... \n",
    "\n",
    "            loss = ...  \n",
    "            total_loss += ... \n",
    "\n",
    "    # Compute and return average loss for this epoch\n",
    "    \n",
    "    return ...\n",
    "\n",
    "def save_checkpoint(model, epoch, path_template):\n",
    "    save_path = path_template.format(epoch=epoch+1)\n",
    "    save_dir = os.path.dirname(save_path)\n",
    "    if save_dir:  \n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"Checkpoint saved: {save_path}\")\n",
    "\n",
    "def plot_losses(train_losses, val_losses):\n",
    "    clear_output()\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(train_losses, label=\"Train Loss\")\n",
    "    plt.plot(val_losses, label=\"Val Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training and Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f420e4-6fa2-4f12-9a1c-4c927d8a3ecd",
   "metadata": {},
   "source": [
    "Для визуализации результатов работы нашей модели нам потребуется несколько вспомогательных функций. Ниже представлены некоторые функции, которые помогут нам подготовить данные, запустить процесс генерации и сравнить сгенерированные изображения с оригиналом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313045d4-e926-4586-a684-b5ddfb53111b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_val_sample(val_loader, device):\n",
    "    img_sample, class_sample = next(iter(val_loader))\n",
    "    img_sample = img_sample[0:1].to(device)\n",
    "    class_sample = class_sample[0:1].to(device)\n",
    "    return img_sample, class_sample\n",
    "\n",
    "def generate_image_tokens(model, class_sample, h, w, device):\n",
    "    num_tokens = h * w\n",
    "    gen_seq = sample(\n",
    "        model,\n",
    "        context=None,\n",
    "        class_label=class_sample,\n",
    "        length=num_tokens,\n",
    "        device=device\n",
    "    ).cpu().numpy().squeeze()\n",
    "    return gen_seq\n",
    "\n",
    "def image_to_tokens(image_tensor, centroids):\n",
    "    token_indices = quantize(image_tensor, centroids).cpu().numpy()\n",
    "    return token_indices.reshape(-1)\n",
    "\n",
    "def tokens_to_image(tokens, h, w, centroids):\n",
    "    tokens_tensor = torch.tensor(tokens, dtype=torch.long).reshape(1, h, w)\n",
    "    rgb_image = unquantize(tokens_tensor, centroids).cpu().numpy().squeeze()\n",
    "    rgb_image = (rgb_image * 255).astype(np.uint8)\n",
    "    return rgb_image\n",
    "\n",
    "def plot_generated_vs_original(gen_img_rgb, original_img_rgb, class_label):\n",
    "    final_img = np.concatenate([gen_img_rgb, original_img_rgb], axis=1)\n",
    "    plt.figure(figsize=(4, 2))\n",
    "    plt.title(f\"Generated vs Original – Class {class_label}\")\n",
    "    plt.imshow(final_img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "def generate_and_plot_sample(model, val_loader, device):\n",
    "    model.eval()\n",
    "    centroids = model.centroids\n",
    "    img_sample, class_sample = pick_val_sample(val_loader, device)\n",
    "    h, w = img_sample.shape[-2:]\n",
    "\n",
    "    gen_seq = generate_image_tokens(model, class_sample, h, w, device)\n",
    "    generated_image_rgb = tokens_to_image(gen_seq, h, w, centroids)\n",
    "    original_tokens = image_to_tokens(img_sample, centroids)\n",
    "    original_image_rgb = tokens_to_image(original_tokens, h, w, centroids)\n",
    "    \n",
    "    plot_generated_vs_original(generated_image_rgb, original_image_rgb, class_sample.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ede196-8661-493f-a368-3bb8658ec847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_imagegpt(model, train_loader, val_loader, optimizer, criterion,\n",
    "                   scheduler, checkpoint_path, device=device, epochs=10):\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    train_loss_history, val_loss_history = [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        train_loss = ...\n",
    "        val_loss = ...\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1}: Train Loss = {train_loss:.4f} | Val Loss = {val_loss:.4f}\")\n",
    "\n",
    "        plot_losses(train_loss_history, val_loss_history)\n",
    "        save_checkpoint(model, epoch, f\"{checkpoint_path}/imagegpt_epoch{epoch}.pt\")\n",
    "        generate_and_plot_sample(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9a7f1e-986d-426f-8829-346449b2913b",
   "metadata": {},
   "source": [
    "Теперь мы готовы запустить основной цикл обучения. Вы можете экспериментировать с различными гиперпараметрами и расписанием обучения, чтобы добиться хороших результатов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89557155-66c6-47e6-acc6-17fbd3603c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs * len(train_loader))\n",
    "checkpoint_path = \"checkpoints_imagegpt\"\n",
    "\n",
    "train_imagegpt(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    scheduler=scheduler,\n",
    "    checkpoint_path=checkpoint_path,\n",
    "    device=device,\n",
    "    epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7753f475",
   "metadata": {},
   "source": [
    "### Задание 8: Autocompletion (1 балл)\n",
    "\n",
    "Ниже реализованы несколько функций для удобной отрисовки результатов. Используйте их, чтобы выполнить задание."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49599759-7fcb-476e-b20a-df04cdf5b3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_val_contexts(val_loader, num_contexts, device):\n",
    "    context_images, class_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, batch_labels in val_loader:\n",
    "            for img, label in zip(images, batch_labels):\n",
    "                context_images.append(img.to(device))\n",
    "                class_labels.append(label.to(device))\n",
    "                if len(context_images) >= num_contexts:\n",
    "                    return context_images, class_labels\n",
    "    return context_images, class_labels\n",
    "\n",
    "def prepare_context(img_tensor, context_size, centroids):\n",
    "    original_tokens = image_to_tokens(img_tensor.unsqueeze(0), centroids).squeeze()\n",
    "    context_tokens = original_tokens[:context_size]\n",
    "    context_tensor = torch.tensor(context_tokens, dtype=torch.long, device=img_tensor.device)\n",
    "    return context_tensor, original_tokens\n",
    "\n",
    "def generate_variations(model, context_tensor, class_label, total_tokens, context_size, samples_per_context, temperature=1.0):\n",
    "    generated_images = []\n",
    "    h = w = int(total_tokens**0.5)\n",
    "    centroids = model.centroids\n",
    "    device = context_tensor.device\n",
    "\n",
    "    for _ in tqdm(range(samples_per_context), desc=\"Generation\", leave=False):\n",
    "        gen_seq = sample(model, \n",
    "                         length=total_tokens - context_size, \n",
    "                         context=context_tensor,\n",
    "                         class_label=class_label, \n",
    "                         temperature=temperature, \n",
    "                         device=device)\n",
    "        \n",
    "        gen_img_rgb = tokens_to_image(gen_seq.cpu().numpy().squeeze(), h, w, centroids)\n",
    "        generated_images.append(gen_img_rgb)\n",
    "    return generated_images\n",
    "\n",
    "def plot_context_rows(rows_of_images, class_labels):\n",
    "    num_contexts = len(rows_of_images)\n",
    "    if num_contexts == 0:\n",
    "        return\n",
    "    num_cols = len(rows_of_images[0])\n",
    "    fig, axs = plt.subplots(nrows=num_contexts, ncols=1, figsize=(num_cols * 2, num_contexts * 2.2))\n",
    "    if num_contexts == 1: \n",
    "        axs = [axs]\n",
    "\n",
    "    for i, (row, label) in enumerate(zip(rows_of_images, class_labels)):\n",
    "        combined_row_img = np.concatenate(row, axis=1)\n",
    "        axs[i].imshow(combined_row_img)\n",
    "        axs[i].set_title(f\"Class: {label}\")\n",
    "        axs[i].axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2c8a7f-721c-499f-912a-3dcc4bf2fa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_plot_variations(model, val_loader, num_contexts=5, samples_per_context=5, context_size=512, temperature=1.0, device='cuda'):\n",
    "    \n",
    "    model.eval()\n",
    "    centroids = model.centroids\n",
    "    \n",
    "    context_images, class_labels = pick_val_contexts(val_loader, num_contexts, device)\n",
    "    h, w = context_images[0].shape[-2:]\n",
    "    total_tokens = h * w\n",
    "    all_rows_for_plotting = []\n",
    "    plot_labels = [label.item() for label in class_labels]\n",
    "\n",
    "    for img, label in tqdm(zip(context_images, class_labels), total=num_contexts):\n",
    "        context_tensor, original_tokens = prepare_context(img, context_size, centroids)\n",
    "        generated_imgs_rgb = generate_variations(\n",
    "            model, context_tensor, label.unsqueeze(0), total_tokens, \n",
    "            context_size, samples_per_context, temperature=temperature\n",
    "        )\n",
    "        context_display_tokens = np.pad(original_tokens[:context_size], \n",
    "                                        (0, total_tokens - context_size), \n",
    "                                        'constant', constant_values=0)\n",
    "        context_img_rgb = tokens_to_image(context_display_tokens, h, w, centroids)\n",
    "        original_img_rgb = tokens_to_image(original_tokens, h, w, centroids)\n",
    "        row = [context_img_rgb] + generated_imgs_rgb + [original_img_rgb]\n",
    "        all_rows_for_plotting.append(row)\n",
    "\n",
    "    plot_context_rows(all_rows_for_plotting, plot_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c859a2b-65e6-40e3-a4a8-e2f44ea31a53",
   "metadata": {},
   "source": [
    "После того как мы обучили модель, мы можем загрузить её, чтобы использовать для генерации. Код ниже загружает веса модели, которые были сохранены во время обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd35bf0e-4184-4b17-8a1a-7993c7213862",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"checkpoints_imagegpt/imagegpt_epoch10.pt\"\n",
    "\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "model.eval()\n",
    "print(f\"Model loaded from: {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0710743-a163-4517-9b78-d8701edda5b1",
   "metadata": {},
   "source": [
    "Теперь давайте перейдем к исследованию способности модели к автодополнению. Ваша задача — проверить, насколько хорошо модель может дорисовывать изображения в зависимости от объёма контекста.\n",
    "\n",
    "Запустить функцию `generate_and_plot_variations` три раза с разными значениями `context_size = {256, 512, 768}`.\n",
    "\n",
    "Обратите внимание, как меняется качество сгенерированных изображений в зависимости от того, сколько контекста получила модель, и сделайте выводы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc84691",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36840777-f239-4ac4-9fe9-8f804626565e",
   "metadata": {},
   "source": [
    "Теперь давайте исследуем влияние температуры на качество и разнообразие изображений. Этот параметр, как мы уже говорили, контролирует \"случайность\" предсказаний модели. \n",
    "\n",
    "Запустите функцию `generate_and_plot_variations` три раза, используя одинаковый размер контекста `context_size=512`, но с разными значениями температуры `temperature = {0.9, 0.7, 0.5}`. Посмотрите на результаты и сделайте выводы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba55f27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659645b9-c5c0-4bdb-a1ec-c8fcb8f4eab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Здесь можно оставить отзывы, пожелания и впечатления о ДЗ :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.mlspace-nick_base]",
   "language": "python",
   "name": "conda-env-.mlspace-nick_base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
