{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6685cc59",
   "metadata": {},
   "source": [
    "## Homework 3: VQ-VAE\n",
    "\n",
    "Ранее мы с вами познакомились с **вариационными автокодировщиками**, которые используют непрерывные скрытые переменные $\\mathbf{z}_e$. Однако для многих типов данных дискретные представления могут быть более естественными.\n",
    "\n",
    "**Vector Quantized Variational Autoencoder** (**VQ-VAE** ) — подход, позволяющий обучить автокодировщики с дискретным латентным пространством. Вместо того чтобы кодировать данные в непрерывное распределение ($\\boldsymbol{\\mu}, \\boldsymbol{\\sigma}^2$), `VQ-VAE` отображает их в один из векторов словаря (кодовой книги).\n",
    "\n",
    "**Основная идея векторной квантизации состоит в следующем**:\n",
    "\n",
    "1. Создается **кодовая книга** (**codebook**) — обучаемый словарь из $K$ векторов-прототипов $\\{\\mathbf{e}_k\\}_{k=1}^K$, где каждый вектор $\\mathbf{e}_k \\in \\mathbb{R}^D$.\n",
    "\n",
    "<center><img src=\"images/codebook.png\" width=300></center>\n",
    "\n",
    "2. **Кодировщик** $q_{\\boldsymbol{\\phi}}(c|\\mathbf{x})$ преобразует входной объект $\\mathbf{x}$ в непрерывное представление $\\mathbf{z}_e \\in \\mathbb{R}^D$.\n",
    "\n",
    "3. Для непрерывного вектора $\\mathbf{z}_e$ находится **ближайший** к нему вектор $\\mathbf{e}_{k^*}$ из кодовой книги:\n",
    "\n",
    "$$k^* = \\arg\\min_k \\|\\mathbf{z}_e - \\mathbf{e}_k\\|$$\n",
    "\n",
    "Результатом такого преобразования является **квантованный вектор** $\\mathbf{z}_q = \\mathbf{e}_{k^*}$.\n",
    "\n",
    "<center><img src=\"images/clusters.png\" width=250></center>\n",
    "\n",
    "4. Квантованный вектор $\\mathbf{z}_q$ передается в **декодер** $p_{\\boldsymbol{\\theta}}(\\mathbf{x}|\\mathbf{z}_q)$ для восстановления исходного объекта $\\mathbf{\\hat{x}}$.\n",
    "\n",
    "В этой работе мы реализуем `VQ-VAE` и исследуем его свойства."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675fc867",
   "metadata": {},
   "source": [
    "### Задание\n",
    "\n",
    "Вам предстоит реализовать модель `VQ-VAE` и обучить модель на датасете `CIFAR10`.\n",
    "\n",
    "За выполнение домашнего задания можно получить до **10 баллов**. Для части заданий мы написали для вас скелет. Заполните в них пропуски, выделенные с помощью `...`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca435ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.datasets import CIFAR10 \n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa56e20",
   "metadata": {},
   "source": [
    "### Задание 1: Dataset (0.5 балла)\n",
    "\n",
    "Для обучения будем использовать датасет `CIFAR-10`.\n",
    "\n",
    "**Ваша задача**:\n",
    "\n",
    "- Загрузить `CIFAR-10`\n",
    "\n",
    "- Создать преобразования `ToTensor` и `Normalize` в диапазон $[-1, 1]$\n",
    "\n",
    "- Создать `Dataset`-ы и `DataLoader`-ы для обучающей и валидационной выборок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853d9876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- YOUR CODE HERE ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b917ef73",
   "metadata": {},
   "source": [
    "### Задание 2: Residual Blocks (0.5 балла)\n",
    "\n",
    "В оригинальной статье `VQ-VAE` авторы использовали стек `ResNet` блоков как в кодировщике, так и в декодере, чтобы построить достаточно глубокие и мощные сверточные сети. \n",
    "\n",
    "Структура блоков имеет следующий вид: `ReLU` $\\rightarrow$ `Conv3x3` $\\rightarrow$ `ReLU` $\\rightarrow$ `Conv1x1`\n",
    "\n",
    "Реализуйте классы `ResidualBlock` и `ResidualStack`, следуя архитектуре из статьи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888870d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, num_hiddens, num_residual_hiddens):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=in_channels,\n",
    "                      out_channels=num_residual_hiddens,\n",
    "                      kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=num_residual_hiddens,\n",
    "                      out_channels=num_hiddens,\n",
    "                      kernel_size=1, stride=1, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Implement the residual connection\n",
    "\n",
    "        return ...\n",
    "    \n",
    "\n",
    "class ResidualBlockStack(nn.Module):\n",
    "    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
    "        super().__init__()\n",
    "        self._num_residual_layers = num_residual_layers\n",
    "        # Create a list self.layers and add num_residual_layers instances of ResidualBlock to the list.\n",
    "        self.layers = ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass x through all layers and apply Relu after the entire stack\n",
    "        return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaf76e1",
   "metadata": {},
   "source": [
    "### Задание 3: Encoder и Decoder (1 балл)\n",
    "\n",
    "**Encoder** и **Decoder** в `VQ-VAE` — это сверточные нейросети, отвечающие за преобразование данных между исходным пространством изображений и пространством непрерывных латентных векторов\n",
    "\n",
    "- **Encoder**:  сжимает входное изображение $\\mathbf{x}$ в латентный вектор $\\mathbf{z}_e$\n",
    "\n",
    "- **Decoder**:  будет принимать на вход квантованный вектор $\\mathbf{z}_q$, полученный из $\\mathbf{z}_e$ после квантизации и восстанавливает из нее изображение $\\hat{\\mathbf{x}}$\n",
    "\n",
    "Ваша задача — реализовать классы `Encoder` и `Decoder`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0683cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define the layers based on the VQ-VAE paper description:\n",
    "        # 1. Conv2d with stride 2 (kernel 4x4, padding 1) -> num_hiddens // 2 channels\n",
    "        self.conv_1 = ...\n",
    "        \n",
    "        # 2. Conv2d with stride 2 (kernel 4x4, padding 1) -> num_hiddens channels\n",
    "        self.conv_2 = ...\n",
    "        \n",
    "        # 3. Conv2d with stride 1 (kernel 3x3, padding 1) -> num_hiddens channels\n",
    "        self.conv_3 = ...\n",
    "        \n",
    "        # 4. ResidualStack \n",
    "        self.residual_stack = ...\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Implement the forward pass:\n",
    "        # Conv1 -> ReLU -> Conv2 -> ReLU -> Conv3 -> ResidualStack\n",
    "        return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8521dbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens, out_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define the layers:\n",
    "        # 1. Conv2d with stride 1 (kernel 3x3, padding 1) -> num_hiddens channels\n",
    "        self.conv_1 = ...\n",
    "        \n",
    "        # 2. ResidualStack\n",
    "        self.residual_stack = ...\n",
    "        \n",
    "        # 3. ConvTranspose2d with stride 2 (kernel 4x4, padding 1) -> num_hiddens // 2 channels\n",
    "        self.conv_trans_1 = ...\n",
    "\n",
    "        # 4. ConvTranspose2d with stride 2 (kernel 4x4, padding 1) -> out_channels\n",
    "        self.conv_trans_2 = ...\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Implement the forward pass:\n",
    "        # Conv1 -> ResidualStack (applies ReLU) -> ConvTranspose1 -> ReLU -> ConvTranspose2\n",
    "        return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61081e6",
   "metadata": {},
   "source": [
    "### Задание 4: VectorQuantizer (2 балла)\n",
    "\n",
    "`VectorQuantizer` — это основной блок `VQ-VAE`, отвечающий за дискретизацию непрерывного латентного пространства.\n",
    "\n",
    "Его основная задача для каждого вектора $\\mathbf{z}_e$ найти индекс $k^*$ ближайшего вектора $\\mathbf{e}_{k^*}$ из **обучаемой кодовой книги**:\n",
    "$\\{\\mathbf{e}_k\\}_{k=1}^K$.$$k^* = \\arg\\min_k \\|\\mathbf{z}_e - \\mathbf{e}_k\\|^2$$\n",
    "\n",
    "И используя найденные индексы, извлечь соответствующие квантованные векторы $\\mathbf{z}_q = \\mathbf{e}_{k^*}$ из кодовой книги, которые и будут передаваться декодеру.\n",
    "\n",
    "Однако проблема в том, что операция `argmin` недифференцируема, поскольку её производная почти везде равна нулю. Чтобы решить эту проблему, в `VQ-VAE` предложили использовать `Straight-Through Estimator`.\n",
    "\n",
    "**Идея STE**:\n",
    "\n",
    "На **прямом проходе** (**forward pass**) мы используем обычный результат операции квантования, т.е. выбираем ближайший вектор $\\mathbf{z}_q = \\mathbf{e}_{k^*}$.\n",
    "\n",
    "На **обратном проходе** (**backward pass**) градиент $\\nabla_{\\mathbf{z}_q} \\mathcal{L}$, который пришел от декодера к $\\mathbf{z}_q$, используется в качестве аппроксимации для градиента по $\\mathbf{z}_e$:\n",
    "\n",
    "$$\\nabla_{\\mathbf{z}_e} \\mathcal{L} \\approx \\nabla_{\\mathbf{z}_q} \\mathcal{L}$$\n",
    "\n",
    "В итоге, градиент от декодера фактически пропускается через недифференцируемый блок квантования без изменений.\n",
    "\n",
    "Хотя `STE` дает смещенную оценку градиента он всё же предоставляет кодировщику полезный обучающий сигнал. Градиент $\\nabla_{\\mathbf{z}_q} \\mathcal{L}$ указывает, в каком направлении должен был бы измениться выбранный вектор $\\mathbf{e}_{k^*}$, чтобы улучшить реконструкцию. Применяя этот же градиент к $\\mathbf{z}_e$, мы подталкиваем выход кодировщика в такое направление, чтобы в следующий раз он с большей вероятностью был квантован в более \"правильный\" вектор из кодовой книги.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f91d04f",
   "metadata": {},
   "source": [
    "### VQ Loss\n",
    "\n",
    "STE решает проблему проброса градиента к кодировщику, однако не предоставляет никакого обучающего сигнала для самой кодовой книги. \n",
    "\n",
    "Поэтому, чтобы обучить кодовую книгу и стабилизировать кодировщик, в `VQ-VAE` вводят два дополнительных лосса: \n",
    "\n",
    "$$L_{VQ} = \\underbrace{\\|\\text{sg}[\\mathbf{z}_e] - \\mathbf{e}_{k^*}\\|^2}_{\\text{Codebook Loss}} + \\underbrace{\\beta \\|\\mathbf{z}_e - \\text{sg}[\\mathbf{e}_{k^*}]\\|^2}_{\\text{Commitment Loss}}$$\n",
    "\n",
    "**Codebook Loss** отвечает за обучение кодовой книги, минимизируя $L_2$-расстояние между выходом кодировщика $\\mathbf{z}_e$ и выбранным вектором $\\mathbf{e}_{k^*}$. Это притягивает вектор $\\mathbf{e}_{k^*}$ к $\\mathbf{z}_e$. Оператор `stop-gradient` у $\\mathbf{z}_e$ блокирует поток градиента к кодировщику. Это важно, так как гарантирует, что данное слагаемое лосса будет обновлять только веса кодовой книги.\n",
    "\n",
    "**Commitment Loss** отвечает за обучение кодировщика, притягивая выход кодировщика $\\mathbf{z}_e$ к выбранному вектору $\\mathbf{e}_{k^*}$. Это заставляет кодировщик **фиксироваться** (**commit**) на выученных векторах словаря и предотвращает неконтролируемый рост его выходов. `Stop-gradient` у $\\mathbf{e}_{k^*}$ блокирует поток градиента к кодовой книге, гарантируя, что это слагаемое будет отвечать только за кодировщик.\n",
    "\n",
    "$\\beta$ — это гиперпараметр, контролирующий силу фиксации."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c31525",
   "metadata": {},
   "source": [
    "### Perplexity\n",
    "\n",
    "Часто для оценки того, насколько эффективно используется кодовая книга используется метрика **perplexity**. Она показывает эффективное количество векторов словаря, которые модель использует в среднем.\n",
    "\n",
    "1. Для батча, состоящего из $N$ векторов $\\mathbf{z}_e$ кодировщик выбирает $N$ индексов $k^*$.\n",
    "\n",
    "2. Вычисляется среднее распределение $p$ использования кодов по этому батчу: $p = (p_1, ..., p_K)$, где $p_k$ — это средняя частота выбора $k$-го вектора из словаря.\n",
    "\n",
    "3. **Perplexity** — это экспонента от энтропии $H(p)$ этого среднего распределения:$$PPL = e^{H(p)} = e^{-\\sum_{k=1}^K p_k \\log p_k}$$\n",
    "\n",
    "**Hint**:\n",
    "\n",
    "Низкая перплексия ($PPL << K$) указывает на **коллапс кодовой книги** (**codebook collapse**), высокая перплексия ($PPL \\approx K$) указывает на хорошее, разнообразное использование словаря."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8520914",
   "metadata": {},
   "source": [
    "Ваша задача — заполнить пропуски в классе `VectorQuantizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5f3246",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizer(nn.Module):\n",
    "\n",
    "    def __init__(self, num_embeddings: int, embedding_dim: int, beta: float):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.beta = beta\n",
    "\n",
    "        self.embedding = nn.Embedding(self.num_embeddings, self.embedding_dim)\n",
    "        self.embedding.weight.data.uniform_(-1./self.num_embeddings, 1./self.num_embeddings)\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Connects the encoder output to the decoder input.\n",
    "        Args:\n",
    "            inputs (Tensor): Input tensor from encoder z_e [B, D, H, W].\n",
    "        Returns:\n",
    "            Tuple[Tensor, Tensor, Tensor]: \n",
    "                vq_loss (scalar = Codebook Loss + beta * Commitment Loss), \n",
    "                quantized_ste (tensor [B, D, H, W] with straight-through gradient), \n",
    "                encoding_indices (tensor [N] (N=B*H*W) for perplexity calculation).\n",
    "        \"\"\"\n",
    "\n",
    "        # Calculate L2 distance between inputs and embedding weights\n",
    "        distances = ...\n",
    "        \n",
    "        # Find the indices of the closest embeddings\n",
    "        encoding_indices = ...  \n",
    "        \n",
    "        # Get the quantized latent vectors using embedding lookup \n",
    "        quantized = ...  \n",
    "        \n",
    "        # Calculate the codebook loss: ||sg[z_e] - e_k*||^2, use detach() for sg\n",
    "\n",
    "        codebook_loss = ...\n",
    "        \n",
    "        # Calculate the commitment loss: ||z_e - sg[e_k*]||^2\n",
    "        commitment_loss = ...\n",
    "        \n",
    "        # Combine the losses\n",
    "        vq_loss = ...\n",
    "        \n",
    "        # Apply the Straight-Through Estimator\n",
    "\n",
    "        quantized_ste = ...                              \n",
    "        \n",
    "        return ...\n",
    "    \n",
    "    def calculate_perplexity(self, encoding_indices: torch.Tensor) -> torch.Tensor:\n",
    "        return ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e779c611",
   "metadata": {},
   "source": [
    "### Задание 5: VQ-VAE (1 балл)\n",
    "\n",
    "Теперь, когда у нас есть `Encoder`, `Decoder` и `VectorQuantizer`, мы можем собрать итоговую модель.\n",
    "\n",
    "Реализуйте класс `VQVAE`, объединив все ранее созданные компоненты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc1643b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQVAE(nn.Module):\n",
    "    def __init__(self, in_channels: int, embedding_dim: int, num_embeddings: int,\n",
    "                 num_hiddens: int, num_residual_layers: int, num_residual_hiddens: int,\n",
    "                 beta: float):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize the encoder\n",
    "        self._encoder = ...\n",
    "\n",
    "        # Initialize the pre-quantization Conv2d with kernel_size=1, stride=1.\n",
    "        self.pre_vq_conv = ...\n",
    "\n",
    "        # Initialize the vector quantization layer\n",
    "        self.vq_layer = ...\n",
    "\n",
    "        # Initialize the decoder\n",
    "        self.decoder = ...\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass through the VQ-VAE model.\n",
    "        Args:\n",
    "            x (Tensor): Input image tensor [B, C_in, H, W].\n",
    "        Returns:\n",
    "            Tuple[Tensor, Tensor, Tensor]:\n",
    "                vq_loss (scalar = Codebook Loss + beta * Commitment Loss),\n",
    "                x_hat_logits (tensor [B, C_in, H, W] - output logits from Decoder),\n",
    "                encoding_indices (tensor [N = B*H'*W'] - flat indices for perplexity calculation).\n",
    "        \"\"\"\n",
    "        # Pass `x` through the encoder to get `z_e`\n",
    "        z_e = ...\n",
    "\n",
    "        # Pass `z_e` through the pre-quantization conv\n",
    "        z_e_pre_vq = ...\n",
    "\n",
    "        # Pass `z_e_pre_vq` through the quantization layer\n",
    "        vq_loss, z_q, encoding_indices = ...\n",
    "\n",
    "        # Pass the quantized vector `z_q` through the decoder\n",
    "        x_hat_logits = ...\n",
    "        \n",
    "        return ...\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def decode_from_indices(self, \n",
    "                            indices: torch.Tensor,\n",
    "                            final_activation=nn.Tanh()) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Takes a grid of indices and decodes them into images.\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        \n",
    "        # Get the quantized vectors `z_q`\n",
    "        z_q = ...\n",
    "            \n",
    "        # Pass `z_q` through the decoder\n",
    "        out_logits = ...\n",
    "        \n",
    "        # Apply the final activation\n",
    "        out = ...\n",
    "            \n",
    "        return out\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def sample(self, \n",
    "               num_samples: int, \n",
    "               latent_shape: Tuple[int, int],\n",
    "               device: torch.device, \n",
    "               final_activation=nn.Tanh()) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Generates images from random codes\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        # Generate random integer indices\n",
    "        indices = ...\n",
    "        \n",
    "        # Use `decode_from_indices` for decoding\n",
    "        return self.decode_from_indices(indices, final_activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a5026f",
   "metadata": {},
   "source": [
    "### Задание 6: Training and Validation Loop (1 балл)\n",
    "\n",
    "Теперь, когда все компоненты модели готовы, мы можем собрать полный цикл обучения. Обучение `VQ-VAE` происходит путем минимизации суммарной функции потерь:\n",
    "$$L_{total} = L_{rec} + L_{VQ},$$\n",
    "\n",
    "- $L_{rec} = \\|\\mathbf{x} - \\mathbf{\\hat{x}}\\|^2$ — лосс, вычисляемый между оригинальным $\\mathbf{x}$ и восстановленным $\\mathbf{\\hat{x}}$ объектами\n",
    "- $L_{VQ} = \\|\\mathbf{z}_e - \\text{sg}[\\mathbf{e}_{k^*}]\\|^2 + \\beta \\|\\text{sg}[\\mathbf{z}_e] - \\mathbf{e}_{k^*}\\|^2$ — лосс, вычисляемый внутри `VectorQuantizer` для обучения кодовой книги и кодировщика.\n",
    "\n",
    "Мы будем отслеживать все компоненты функции потерь (`total`, `reconstruction`, `vq`) и `Perplexity` на обучающей и валидационной выборках, чтобы контролировать процесс обучения.\n",
    "\n",
    "Реализуйте функции `train_step_vqvae`, `validate_step_vqvae` и основной цикл `train_loop_vqvae`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169555d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step_vqvae(model, optimizer, train_loader, device):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    epoch_losses = {'loss': 0.0, 'recon_loss': 0.0, 'vq_loss': 0.0, 'perplexity': 0.0}\n",
    "    num_batches = len(train_loader)\n",
    "\n",
    "    for batch_x, _ in tqdm(train_loader, desc=\"Train\", leave=False):\n",
    "        x = batch_x.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Get model outputs\n",
    "        ...\n",
    "\n",
    "        # Apply final activation\n",
    "        ...\n",
    "\n",
    "        # Calculate reconstruction loss, total loss, perplexity\n",
    "        ...\n",
    "\n",
    "        # Get average losses over batches\n",
    "       \n",
    "    return epoch_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088753d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate_step_vqvae(model, val_loader, device):\n",
    "    \n",
    "    model.eval()\n",
    "    epoch_losses = {'loss': 0.0, 'recon_loss': 0.0, 'vq_loss': 0.0, 'perplexity': 0.0}\n",
    "    num_batches = len(val_loader)\n",
    "\n",
    "    for batch_x, _ in tqdm(val_loader, desc=\"Val\", leave=False):\n",
    "        x = batch_x.to(device)\n",
    "\n",
    "        # --- YOUR CODE HERE ---\n",
    "        \n",
    "    return epoch_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f96593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_reconstructions(model, data_loader, device, epoch, num_images=10, final_activation=F.tanh):\n",
    "    model.eval()\n",
    "    \n",
    "    x_batch, _ = next(iter(data_loader))\n",
    "    originals = x_batch[:num_images].to(device)\n",
    "\n",
    "    _, x_hat_logits, _ = model(originals)\n",
    "    reconstructions = final_activation(x_hat_logits)\n",
    "    \n",
    "    originals_cpu = originals.cpu()\n",
    "    reconstructions_cpu = reconstructions.cpu()\n",
    "\n",
    "    images_to_plot = torch.cat([originals_cpu, reconstructions_cpu], dim=0)\n",
    "    grid = make_grid(images_to_plot * 0.5 + 0.5, nrow=num_images)\n",
    "    \n",
    "    plt.figure(figsize=(num_images * 1.5, 4))\n",
    "    plt.imshow(grid.permute(1, 2, 0))\n",
    "    plt.title(f\"Reconstructions: Epoch {epoch} (Top: Original, Bottom: Reconstructed)\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7227d303",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_losses(train_stats, val_stats):\n",
    "    clear_output(wait=True)\n",
    "    epochs = range(1, len(train_stats['loss']) + 1)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    axes = axes.flatten() \n",
    "    \n",
    "    metrics_to_plot = [\n",
    "        ('loss', 'Total Loss'), \n",
    "        ('recon_loss', 'Reconstruction Loss'), \n",
    "        ('vq_loss', 'VQ + Commitment Loss'), \n",
    "        ('perplexity', 'Perplexity')\n",
    "    ]\n",
    "    \n",
    "    for ax, (key, title) in zip(axes, metrics_to_plot):\n",
    "        ax.plot(epochs, train_stats[key], label=f'Train {title}')\n",
    "        ax.plot(epochs, val_stats[key], label=f'Validation {title}')\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylabel('Value')\n",
    "        ax.grid(True)\n",
    "        ax.legend()\n",
    "        \n",
    "    fig.suptitle('Training VQ-VAE', fontsize=14)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "    \n",
    "def save_checkpoint(model, optimizer, epoch, path_template):\n",
    "    save_path = path_template.format(epoch=epoch)\n",
    "    save_dir = os.path.dirname(save_path)\n",
    "    if save_dir:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, save_path)\n",
    "\n",
    "    print(f\"Checkpoint saved: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1cdcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_vqvae(model, optimizer, train_loader, val_loader, num_epochs, device,\n",
    "                     checkpoint_path=None):\n",
    "    model = model.to(device)\n",
    "    train_history = {'loss': [], 'recon_loss': [], 'vq_loss': [], 'perplexity': []}\n",
    "    val_history = {'loss': [], 'recon_loss': [], 'vq_loss': [], 'perplexity': []}\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_epoch_stats = ...\n",
    "        val_epoch_stats = ...\n",
    "\n",
    "        for key in train_history:\n",
    "            ...\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        plot_all_losses(train_history, val_history) \n",
    "        \n",
    "        visualize_reconstructions(model, val_loader, device, epoch) \n",
    "\n",
    "        print(f\"[Epoch {epoch}/{num_epochs}]\")\n",
    "        print(f\"  Train: Loss={train_epoch_stats['loss']:.4f}, Recon={train_epoch_stats['recon_loss']:.4f}, VQ={train_epoch_stats['vq_loss']:.4f}, PPL={train_epoch_stats['perplexity']:.2f}\")\n",
    "        print(f\"  Val:   Loss={val_epoch_stats['loss']:.4f}, Recon={val_epoch_stats['recon_loss']:.4f}, VQ={val_epoch_stats['vq_loss']:.4f}, PPL={val_epoch_stats['perplexity']:.2f}\")\n",
    "\n",
    "        save_checkpoint(model, optimizer, epoch, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ac60bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn_recon = nn.MSELoss() \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_epochs = 300\n",
    "learning_rate = 5e-4\n",
    "\n",
    "in_channels = 3\n",
    "embedding_dim = 64\n",
    "num_embeddings = 512\n",
    "num_hiddens = 256\n",
    "num_residual_layers = 2\n",
    "num_residual_hiddens = 64\n",
    "beta = 0.25\n",
    "\n",
    "model = VQVAE(\n",
    "        in_channels=in_channels, \n",
    "        embedding_dim=embedding_dim, \n",
    "        num_embeddings=num_embeddings, \n",
    "        num_hiddens=num_hiddens, \n",
    "        num_residual_layers=num_residual_layers, \n",
    "        num_residual_hiddens=num_residual_hiddens, \n",
    "        beta=beta\n",
    "    )\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "checkpoint_path = \"checkpoints_vq/vqvae_epoch_{epoch}.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86686d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loop_vqvae(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        num_epochs=num_epochs,\n",
    "        device=device,\n",
    "        checkpoint_path=checkpoint_path\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978fb7b4",
   "metadata": {},
   "source": [
    "### Задание 7: Sampling (0.5 балла)\n",
    "\n",
    "После обучения `VQ-VAE` мы можем использовать его декодер для генерации новых изображений. Давайте попробуем это сделать, загрузив обученную модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c497b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vqvae_checkpoint_path = 'checkpoints_vq/vqvae_epoch_300.pth' \n",
    "\n",
    "model_vqvae = VQVAE(\n",
    "    in_channels=3,\n",
    "    embedding_dim=64,\n",
    "    num_embeddings=512,\n",
    "    num_hiddens=256,\n",
    "    num_residual_layers=2,\n",
    "    num_residual_hiddens=64,\n",
    "    beta=0.25\n",
    ").to(device)\n",
    "\n",
    "checkpoint = torch.load(vqvae_checkpoint_path, map_location=device)\n",
    "model_vqvae.load_state_dict(checkpoint['model_state_dict'])\n",
    "model_vqvae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980aed7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def visualize_generation(model, device, latent_shape=(8, 8), num_images=50, nrow=10):\n",
    "    model.eval()\n",
    "    \n",
    "    samples = model.sample(num_images, latent_shape, device).cpu()\n",
    "    \n",
    "    grid = make_grid(samples * 0.5 + 0.5, nrow=nrow)\n",
    "    \n",
    "    plt.figure(figsize=(nrow*1.5, (num_images//nrow)*1.7 ))\n",
    "    plt.imshow(grid.permute(1, 2, 0))\n",
    "    plt.title(f\"Generated Samples\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c8e3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_generation(model_vqvae, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b81635",
   "metadata": {},
   "source": [
    "**Вопрос**:\n",
    "\n",
    "Почему сгенерированные изображения имеют такой вид?\n",
    "Как это можно исправить?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b718cf",
   "metadata": {},
   "source": [
    "**Ваш ответ**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e040a74",
   "metadata": {},
   "source": [
    "### Learnable Prior\n",
    "\n",
    "Итак, мы реализовали и обучили модель `VQ-VAE`. Она хорошо обучилась сжимать изображения в дискретное латентное пространство, а затем их восстанавливать. Однако, как вы могли заметить сама по себе `VQ-VAE` не является хорошей генеративной моделью. \n",
    "\n",
    "Чтобы `VQ-VAE` могла генерировать качественные изображения, нам необходимо научиться моделировать априорное распределение $p(\\mathbf{z})$ над этими дискретными скрытыми переменными. То есть, нам нужна модель, которая понимает, какие последовательности индексов являются осмысленными.\n",
    "\n",
    "Для этой задачи отлично подходят авторегрессионные модели. Авторы оригинальной статьи предложили использовать для этого модель `Gated PixelCNN`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b1bfa9",
   "metadata": {},
   "source": [
    "### Задание 8: Latent Dataset (1 балл)\n",
    "\n",
    "Поскольку PixelCNN будет работать не в пространстве пикселей, а в пространстве дискретных латентных переменных, нам необходимо предварительно создать этот датасет.\n",
    "\n",
    "**Ваша задача**:\n",
    "\n",
    "- Загрузить обученную модель VQ-VAE\n",
    "\n",
    "- Реализовать функцию `get_code_indices`, которая прогоняет датасет изображений через кодировщик и `Vector Quantizer` и возвращает сетку индексов кодовой книги для каждого изображения\n",
    "\n",
    "- Создать и сохранить датасеты индексов для обучающей и валидационной выборок `CIFAR-10`\n",
    "\n",
    "- Реализовать класс `LatentCodeDataset` для загрузки полученных данных\n",
    "\n",
    "- Создать `DataLoader`-ы для обучающей и валидационной выборок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8345677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- YOUR CODE HERE ---\n",
    "\n",
    "vqvae_model = ...\n",
    "train_prior_loader = ...\n",
    "val_prior_loader = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea38282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_code_indices(model, data_loader, device):\n",
    "    all_indices = []\n",
    "    all_labels = []\n",
    "    model.eval()\n",
    "\n",
    "    for images, labels in tqdm(data_loader, desc=\"Encoding\"):\n",
    "        images = images.to(device)\n",
    "\n",
    "        # Pass images through the encoder\n",
    "        z_e = ...\n",
    "\n",
    "        # Pass z_e through the pre_vq_conv layer\n",
    "        z_e_pre_vq = ...\n",
    "\n",
    "        # Get the encoding indices from the vq_layer.\n",
    "        ...\n",
    "\n",
    "        # Append results \n",
    "    \n",
    "    return ..., ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49eb4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentCodeDataset(Dataset):\n",
    "    def __init__(self, data_path):\n",
    "        ...\n",
    "\n",
    "    def __len__(self):\n",
    "        return ...\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5439cfb",
   "metadata": {},
   "source": [
    "### Задание 9: Gated PixelCNN (1.5 балла)\n",
    "\n",
    "Итак, чтобы `VQ-VAE` мог генерировать качественные изображения, нам необходимо смоделировать априорное распределение $p(\\mathbf{z})$. Мы будем делать это с помощью авторегрессионной модели `Gated PixelCNN`. Вот её основные идеи:\n",
    "\n",
    "- **Маскированные cвертки** (**Masked Convolutions**) видят только контекст (пиксели/коды выше и левее). `Маска A` применяется к первому слою, `маска B` — ко всем последующим.\n",
    "\n",
    "- Для эффективного вычисления, в `Gated PixelCNN` используется два потока сверток:\n",
    "    - **Вертикальный поток**: видит все строки над текущей\n",
    "    - **Горизонтальный поток**: видит пиксели левее в текущей строке, а также обуславливается выходом вертикального потока\n",
    "\n",
    "- Вместо стандартной **ReLU** используется **Gated Activations** $\\mathbf{y} = \\tanh(W_f * \\mathbf{x}) \\odot \\sigma(W_g * \\mathbf{x})$, что, как было показано авторами, значительно улучшает производительность.\n",
    "\n",
    "<center><img src=\"images/GatedPixelCNN.png\" width=350></center>\n",
    "\n",
    "В этом задании вам нужно реализовать классы `GatedMaskedConv2d` и `GatedPixelCNN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c7c548",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedActivation(nn.Module):\n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        # Split the input tensor `x` into two halves along the channel dimension (dim=1)\n",
    "        \n",
    "        # Apply tanh to the first half and sigmoid to the second half and return their element-wise product\n",
    "\n",
    "        return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e15bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedMaskedConv2d(nn.Module):\n",
    "    def __init__(self, mask_type, dim, kernel_size, residual=True, n_classes=None):\n",
    "        super().__init__()\n",
    "        assert kernel_size % 2 == 1, \"Kernel size must be odd\"\n",
    "        self.mask_type = mask_type\n",
    "        self.n_classes = n_classes\n",
    "        self.residual = residual\n",
    "        \n",
    "        # Conditional embedding\n",
    "        if self.n_classes is not None: \n",
    "            self.class_cond_embedding = nn.Embedding(n_classes, 2*dim)\n",
    "        \n",
    "        # Vertical stack convolutions\n",
    "        ks_v = (kernel_size//2 + 1, kernel_size)\n",
    "        pad_v = (kernel_size//2, kernel_size//2)\n",
    "        self.vert_stack = nn.Conv2d(dim, dim*2, ks_v, 1, pad_v)\n",
    "        self.vert_to_horiz = nn.Conv2d(2*dim, 2*dim, 1)\n",
    "        \n",
    "        # Horizontal stack convolutions\n",
    "        ks_h = (1, kernel_size//2 + 1)\n",
    "        pad_h = (0, kernel_size//2)\n",
    "        self.horiz_stack = nn.Conv2d(dim, dim*2, ks_h, 1, pad_h)\n",
    "        self.horiz_resid = nn.Conv2d(dim, dim, 1)\n",
    "        \n",
    "        self.gate = GatedActivation()\n",
    "\n",
    "    def make_causal(self):\n",
    "        \"\"\"Applies Mask 'A' to the convolutional weights.\"\"\"\n",
    "        \n",
    "        # Zero out the bottom row of the vertical stack's kernel\n",
    "        ...\n",
    "        \n",
    "        # Zero out the right-most column of the horizontal stack's kernel\n",
    "        ...\n",
    "\n",
    "    def forward(self, x_v, x_h, y=None):\n",
    "        if self.mask_type == 'A': \n",
    "            self.make_causal()\n",
    "        \n",
    "        y_cond = 0\n",
    "        if y is not None and self.class_cond_embedding is not None:\n",
    "            y_cond = self.class_cond_embedding(y).unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "        # 1. Vertical stack:\n",
    "        #    Apply self.vert_stack to x_v\n",
    "        #    Add y_cond\n",
    "        #    Apply self.gate to get out_v\n",
    "        out_v = ...\n",
    "\n",
    "        # 2. Horizontal stack:\n",
    "        #    Apply self.vert_to_horiz to h_vert\n",
    "        #    Apply self.horiz_stack to x_h\n",
    "        #    Sum all components: vert_to_horiz + horiz_stack + y_cond\n",
    "        #    Apply self.gate to get out\n",
    "        out = ...\n",
    "\n",
    "        # 3. Residual connection for the horizontal stream:\n",
    "        if self.residual:\n",
    "            out_h = ...\n",
    "        else:\n",
    "            out_h = ...\n",
    "\n",
    "        return out_v, out_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74d0675",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedPixelCNN(nn.Module):\n",
    "    def __init__(self, num_embeddings=512, embedding_dim=128, n_layers=15, n_classes=10):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "\n",
    "        # Create the embedding layer \n",
    "        self.embedding = ...\n",
    "\n",
    "        for i in range(n_layers):\n",
    "            mask_type = 'A' if i == 0 else 'B'\n",
    "            kernel_size = 7 if i == 0 else 3\n",
    "            residual = False if i == 0 else True\n",
    "            self.layers.append(\n",
    "                GatedMaskedConv2d(mask_type, embedding_dim, kernel_size, residual, n_classes)\n",
    "            )\n",
    "\n",
    "        # Create the final output convolution layers \n",
    "        #    nn.Sequential: Conv1x1 (D -> 512) -> ReLU -> Conv1x1 (512 -> K)\n",
    "        self.output_conv = ...\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        # Embed the input indices x \n",
    "        # x_emb = ...\n",
    "\n",
    "        # Initialize vertical and horizontal streams\n",
    "        x_v, x_h = ...\n",
    "\n",
    "        # Pass through all layers\n",
    "        ...\n",
    "\n",
    "        # Apply the final output convolutions to `x_h` to get logits\n",
    "        logits = ...\n",
    "        \n",
    "        return logits\n",
    "        \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, h=None, shape=(8, 8), num_samples=10, device='gpu'):\n",
    "        self.eval()\n",
    "        latents = torch.zeros((num_samples, *shape), dtype=torch.int64).to(device)\n",
    "        \n",
    "        if h is not None:\n",
    "             h = h.to(device)\n",
    "        \n",
    "        for i in range(shape[0]):\n",
    "            for j in range(shape[1]):\n",
    "\n",
    "                latents = ...\n",
    "                \n",
    "        return latents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ae6bff",
   "metadata": {},
   "source": [
    "### Задание 10: Training and Validation Loop for PixelCNN (0.5 балла)\n",
    "\n",
    "Реализуйте функции `train_prior`, `validate_prior` и основной цикл `train_loop_prior`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea48474",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_prior(model, optimizer, train_loader, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for codes, labels in tqdm(train_loader, desc=\"Prior Train\", leave=False):\n",
    "        codes, labels = codes.to(device), labels.to(device)\n",
    "\n",
    "        # --- YOUR CODE HERE ---\n",
    "\n",
    "    return ...\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_prior(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for codes, labels in tqdm(val_loader, desc=\"Prior Val\", leave=False):\n",
    "        codes, labels = codes.to(device), labels.to(device)\n",
    "\n",
    "        # --- YOUR CODE HERE ---\n",
    "\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f930202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prior_losses(train_losses, val_losses):\n",
    "    clear_output(wait=True) \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Cross Entropy')\n",
    "    plt.title('Loss Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "def visualize_prior_generation(prior_model, vqvae_model, device, shape=(8, 8), n_classes=10):\n",
    "    prior_model.eval()\n",
    "    vqvae_model.eval()\n",
    "    \n",
    "    labels_to_generate = torch.arange(n_classes).to(device)\n",
    "\n",
    "    latent_codes = prior_model.sample(labels_to_generate, shape=shape, \n",
    "                                        batch_size=n_classes, device=device)\n",
    "    \n",
    "    generated_images = vqvae_model.decode_from_indices(latent_codes).cpu()\n",
    "    \n",
    "    grid = make_grid(generated_images * 0.5 + 0.5, nrow=n_classes) \n",
    "    plt.figure(figsize=(n_classes * 1.5, 3))\n",
    "    plt.imshow(grid.permute(1, 2, 0))\n",
    "    plt.title(f'Generated Images with Prior')\n",
    "    plt.axis('off') \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ec9631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_prior(prior_model, vqvae_model, optimizer, train_loader, val_loader, criterion, num_epochs, device):\n",
    "    prior_model.to(device)\n",
    "    vqvae_model.to(device) \n",
    "    train_losses, val_losses = [], []\n",
    "    latent_shape = next(iter(train_loader))[0].shape[1:] \n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        avg_train_loss = ...\n",
    "        avg_val_loss = ...\n",
    "        \n",
    "        # --- YOUR CODE HERE ---\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        plot_prior_losses(train_losses, val_losses)\n",
    "        visualize_prior_generation(prior_model, vqvae_model, device, shape=latent_shape)\n",
    "        \n",
    "        print(f\"[Epoch {epoch}/{num_epochs}] Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d8a915",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixelcnn_embedding_dim = 128\n",
    "pixelcnn_n_layers = 12 \n",
    "\n",
    "prior_num_epochs = 10\n",
    "prior_learning_rate = 1e-4\n",
    "\n",
    "\n",
    "prior_model = GatedPixelCNN(\n",
    "        num_embeddings=num_embeddings, \n",
    "        embedding_dim=pixelcnn_embedding_dim, \n",
    "        n_layers=pixelcnn_n_layers, \n",
    "        n_classes=10\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae92d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(prior_model.parameters(), lr=prior_learning_rate)\n",
    "\n",
    "train_loop_prior(prior_model, vqvae_model, optimizer, train_prior_loader, val_prior_loader, \n",
    "                     criterion, prior_num_epochs, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b0dc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_prior_generation(prior_model, vqvae_model, device, shape=(8, 8), n_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5b736d",
   "metadata": {},
   "source": [
    "### Задание 11: Выводы (0.5 балла)\n",
    "\n",
    "Сгенерируйте 50 изображений с помощью функции `visualize_prior_generation`, посмотрите на них и сравните их с теми, что вы получили в **Задании 7**.\n",
    "\n",
    "Ответьте на следующие вопросы:\n",
    "\n",
    "1. Сравните качество изображений, сгенерированных с помощью **learnable prior**, и изображений, полученных путем наивного сэмплирования. В чем разница?\n",
    "\n",
    "2. Почему обучение PixelCNN поверх дискретных кодов позволяет генерировать осмысленные изображения? Объясните роль **prior** $p(\\mathbf{z})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed52b79c",
   "metadata": {},
   "source": [
    "**Ваш ответ**:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.mlspace-nick_base]",
   "language": "python",
   "name": "conda-env-.mlspace-nick_base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
